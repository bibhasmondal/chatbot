{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update_progress() : Displays or updates a console progress bar\n",
    "## Accepts a float between 0 and 1. Any int will be converted to a float.\n",
    "## A value under 0 represents a 'halt'.\n",
    "## A value at 1 or bigger represents 100%\n",
    "def update_progress(progress):\n",
    "    barLength = 10 # Modify this to change the length of the progress bar\n",
    "    status = \"\"\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "        status = \"error: progress var must be float\\r\\n\"\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "        status = \"Halt...\\r\\n\"\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "        status = \"Done...\\r\\n\"\n",
    "    block = int(round(barLength*progress))\n",
    "    text = \"\\rPercent: [{:s}] {:.2f}% {:s}\".format( \":\"*block + \"-\"*(barLength-block), progress*100, status)\n",
    "    sys.stdout.write(text)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transalation\n",
    "class ChatbotTrainDataset(Dataset):\n",
    "    class Voc(object):\n",
    "        def __init__(self, name):\n",
    "            self.name = name\n",
    "            self.word2index = {}\n",
    "            self.word2count = {}\n",
    "            self.index2word = {0: \"SOS\", 1: \"EOS\", 2:\"PAD\"}\n",
    "            self.n_words = 3  # Count SOS and EOS\n",
    "\n",
    "        def add_sentence(self, sentence):\n",
    "            for word in sentence.split(' '):\n",
    "                self.add_word(word)\n",
    "\n",
    "        def add_word(self, word):\n",
    "            if word not in self.word2index:\n",
    "                self.word2index[word] = self.n_words\n",
    "                self.word2count[word] = 1\n",
    "                self.index2word[self.n_words] = word\n",
    "                self.n_words += 1\n",
    "            else:\n",
    "                self.word2count[word] += 1\n",
    "                \n",
    "    def __init__(self,lang1,lang2,MAX_LENGTH = 10,reverse=False):\n",
    "        self.MAX_LENGTH = MAX_LENGTH\n",
    "        self.SOS_token = 0\n",
    "        self.EOS_token = 1\n",
    "        self.PAD_token = 2\n",
    "        self.input_voc,self.output_voc,self.pairs = self.prepare_data(lang1,lang2,reverse)\n",
    "        input_data = list(map(lambda x:self.indexes_from_sentence(self.input_voc,x[0])+[self.EOS_token],self.pairs))\n",
    "        output_data = list(map(lambda x:self.indexes_from_sentence(self.input_voc,x[0])+[self.EOS_token],self.pairs))\n",
    "        self.input_lengths = np.array([len(seq) for seq in input_data])\n",
    "        self.output_lengths = np.array([len(seq) for seq in output_data])\n",
    "        self.input_data = self.zeroPadding(input_data,self.PAD_token)\n",
    "        self.output_data = self.zeroPadding(output_data,self.PAD_token)       \n",
    "        \n",
    "    def filter_pair(self, p):\n",
    "        return len(p[0].split(' ')) < self.MAX_LENGTH and \\\n",
    "               len(p[1].split(' ')) < self.MAX_LENGTH \n",
    "\n",
    "    def filter_pairs(self, pairs):\n",
    "        return [pair for pair in pairs if self.filter_pair(pair)]\n",
    "\n",
    "    @staticmethod\n",
    "    def unicode_to_ascii(s):\n",
    "        return ''.join(\n",
    "            c for c in unicodedata.normalize('NFD', s)\n",
    "            if unicodedata.category(c) != 'Mn'\n",
    "        )\n",
    "\n",
    "    def normalize_string(self, s):\n",
    "        s = self.unicode_to_ascii(s.strip()).lower().strip()        \n",
    "        s = re.sub(\"([.!?])\", \" \\1\", s)\n",
    "        s = re.sub(\"[^a-zA-Z.!?s]+\", \" \", s)\n",
    "        return s.strip()\n",
    "\n",
    "    def read_lang(self, lang1, lang2, reverse=False):\n",
    "        print(\"Reading lines...\")\n",
    "        # combine every two lines into pairs and normalize\n",
    "        with open('../data/interim/%s-%s.txt' % (lang1, lang2), encoding='utf-8') as f:\n",
    "            content = f.readlines()\n",
    "        lines = [x.strip() for x in content]\n",
    "        pairs = [[self.normalize_string(s) for s in line.split('\\t')] for line in lines]\n",
    "        if reverse:\n",
    "            pairs = [list(reversed(p)) for p in pairs]\n",
    "            input_voc = self.Voc(lang2)\n",
    "            output_voc = self.Voc(lang1)\n",
    "        else:\n",
    "            input_voc = self.Voc(lang1)\n",
    "            output_voc = self.Voc(lang2)\n",
    "        return input_voc, output_voc, pairs\n",
    "\n",
    "    def indexes_from_sentence(self, voc, sentence):\n",
    "        return [voc.word2index[word] for word in sentence.split(' ')]\n",
    "    \n",
    "    # batch_first: true -> false, i.e. shape: seq_len * batch\n",
    "    def zeroPadding(self,data, fillvalue):\n",
    "        pad = len(max(data, key=len))\n",
    "        return np.array([i + [fillvalue]*(pad-len(i)) for i in data])\n",
    "\n",
    "    def prepare_data(self, lang1, lang2, reverse=False):\n",
    "        input_voc, output_voc, pairs = self.read_lang(lang1, lang2, reverse)\n",
    "        print(\"Read %s sentence pairs\" % len(pairs))\n",
    "        pairs = self.filter_pairs(pairs)\n",
    "        print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "        print(\"Counting words...\")\n",
    "        for pair in pairs:\n",
    "            input_voc.add_sentence(pair[0])\n",
    "            output_voc.add_sentence(pair[1])\n",
    "        print(\"Counted words:\")\n",
    "        print(input_voc.name, input_voc.n_words)\n",
    "        print(output_voc.name, output_voc.n_words)\n",
    "        return input_voc,output_voc,pairs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_data[idx],self.input_lengths[idx],self.output_data[idx],self.output_lengths[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation\n",
    "import pandas as pd\n",
    "class ChatbotTrainDataset(Dataset):\n",
    "    class Voc(object):\n",
    "        def __init__(self, name):\n",
    "            self.name = name\n",
    "            self.word2index = {}\n",
    "            self.word2count = {}\n",
    "            self.index2word = {0: \"SOS\", 1: \"EOS\", 2:\"PAD\"}\n",
    "            self.n_words = 3  # Count SOS and EOS\n",
    "\n",
    "        def add_sentence(self, sentence):\n",
    "            for word in sentence.split(' '):\n",
    "                self.add_word(word)\n",
    "\n",
    "        def add_word(self, word):\n",
    "            if word not in self.word2index:\n",
    "                self.word2index[word] = self.n_words\n",
    "                self.word2count[word] = 1\n",
    "                self.index2word[self.n_words] = word\n",
    "                self.n_words += 1\n",
    "            else:\n",
    "                self.word2count[word] += 1\n",
    "                \n",
    "    def __init__(self,lang1,lang2,MAX_LENGTH = 100,reverse=False):\n",
    "        self.MAX_LENGTH = MAX_LENGTH\n",
    "        self.SOS_token = 0\n",
    "        self.EOS_token = 1\n",
    "        self.PAD_token = 2\n",
    "        self.input_voc,self.output_voc,self.pairs = self.prepare_data(lang1,lang2,reverse)\n",
    "        input_data = list(map(lambda x:self.indexes_from_sentence(self.input_voc,x[0])+[self.EOS_token],self.pairs))\n",
    "        output_data = list(map(lambda x:self.indexes_from_sentence(self.input_voc,x[0])+[self.EOS_token],self.pairs))\n",
    "        self.input_lengths = np.array([len(seq) for seq in input_data])\n",
    "        self.output_lengths = np.array([len(seq) for seq in output_data])\n",
    "        self.input_data = self.zeroPadding(input_data,self.PAD_token)\n",
    "        self.output_data = self.zeroPadding(output_data,self.PAD_token)       \n",
    "        \n",
    "    def filter_pair(self, p):\n",
    "        return len(p[0].split(' ')) < self.MAX_LENGTH and \\\n",
    "               len(p[1].split(' ')) < self.MAX_LENGTH \n",
    "\n",
    "    def filter_pairs(self, pairs):\n",
    "        return [pair for pair in pairs if self.filter_pair(pair)]\n",
    "\n",
    "    @staticmethod\n",
    "    def unicode_to_ascii(s):\n",
    "        return ''.join(\n",
    "            c for c in unicodedata.normalize('NFD', s)\n",
    "            if unicodedata.category(c) != 'Mn'\n",
    "        )\n",
    "\n",
    "    def normalize_string(self, s):\n",
    "        s = self.unicode_to_ascii(s.strip()).lower().strip()        \n",
    "        s = re.sub(r\"\\n\", \"\",  s)\n",
    "        s = re.sub(r\"[-()]\", \"\", s)\n",
    "        s = re.sub(r\"\\.\", \" .\", s)\n",
    "        s = re.sub(r\"\\!\", \" !\", s)\n",
    "        s = re.sub(r\"\\?\", \" ?\", s)\n",
    "        s = re.sub(r\"\\,\", \" ,\", s)\n",
    "        s = re.sub(r\"i'm\", \"i am\", s)\n",
    "        s = re.sub(r\"he's\", \"he is\", s)\n",
    "        s = re.sub(r\"she's\", \"she is\", s)\n",
    "        s = re.sub(r\"it's\", \"it is\", s)\n",
    "        s = re.sub(r\"that's\", \"that is\", s)\n",
    "        s = re.sub(r\"what's\", \"that is\", s)\n",
    "        s = re.sub(r\"\\'ll\", \" will\", s)\n",
    "        s = re.sub(r\"\\'re\", \" are\", s)\n",
    "        s = re.sub(r\"won't\", \"will not\", s)\n",
    "        s = re.sub(r\"can't\", \"cannot\", s)\n",
    "        s = re.sub(r\"n't\", \" not\", s)\n",
    "        s = re.sub(r\"n'\", \"ng\", s)\n",
    "        s = re.sub(r\"ohh\", \"oh\", s)\n",
    "        s = re.sub(r\"ohhh\", \"oh\", s)\n",
    "        s = re.sub(r\"ohhhh\", \"oh\", s)\n",
    "        s = re.sub(r\"ohhhhh\", \"oh\", s)\n",
    "        s = re.sub(r\"ohhhhhh\", \"oh\", s)\n",
    "        s = re.sub(r\"ahh\", \"ah\", s)\n",
    "        return s.strip()\n",
    "\n",
    "    def read_lang(self, lang1, lang2, reverse=False):\n",
    "        print(\"Reading lines...\")\n",
    "        # combine every two lines into pairs and normalize\n",
    "        lines = pd.read_csv(\"../data/processed/All-seasons.csv\").Line\n",
    "        lines = iter(lines)\n",
    "        pairs = [[self.normalize_string(line),self.normalize_string(next(lines))] for line in lines]\n",
    "        if reverse:\n",
    "            pairs = [list(reversed(p)) for p in pairs]\n",
    "            input_voc = self.Voc(lang2)\n",
    "            output_voc = self.Voc(lang1)\n",
    "        else:\n",
    "            input_voc = self.Voc(lang1)\n",
    "            output_voc = self.Voc(lang2)\n",
    "        return input_voc, output_voc, pairs\n",
    "\n",
    "    def indexes_from_sentence(self, voc, sentence):\n",
    "        return [voc.word2index[word] for word in sentence.split(' ')]\n",
    "    \n",
    "    # batch_first: true -> false, i.e. shape: seq_len * batch\n",
    "    def zeroPadding(self,data, fillvalue):\n",
    "        pad = len(max(data, key=len))\n",
    "        return np.array([i + [fillvalue]*(pad-len(i)) for i in data])\n",
    "\n",
    "    def prepare_data(self, lang1, lang2, reverse=False):\n",
    "        input_voc, output_voc, pairs = self.read_lang(lang1, lang2, reverse)\n",
    "        print(\"Read %s sentence pairs\" % len(pairs))\n",
    "        pairs = self.filter_pairs(pairs)\n",
    "        print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "        print(\"Counting words...\")\n",
    "        for pair in pairs:\n",
    "            input_voc.add_sentence(pair[0])\n",
    "            output_voc.add_sentence(pair[1])\n",
    "        print(\"Counted words:\")\n",
    "        print(input_voc.name, input_voc.n_words)\n",
    "        print(output_voc.name, output_voc.n_words)\n",
    "        return input_voc,output_voc,pairs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_data[idx],self.input_lengths[idx],self.output_data[idx],self.output_lengths[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 35448 sentence pairs\n",
      "Trimmed to 35052 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 21290\n",
      "fra 21153\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ChatbotTrainDataset('eng', 'fra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset,batch_size=50,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "702"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-8e2c91c95a4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\ml\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\ml\\lib\\site-packages\\torch\\utils\\data\\sampler.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\ml\\lib\\site-packages\\torch\\utils\\data\\sampler.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_source\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "list(DataLoader(None,batch_size=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicEncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, embed_size,hidden_size, n_layers=1):\n",
    "        super(DynamicEncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, embed_size)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size,n_layers,bidirectional=True)\n",
    "\n",
    "    def forward(self,input_seqs, input_lens, hidden=None):\n",
    "        batch_size = input_seqs.size(0)\n",
    "        input_lens,sort_idx = input_lens.sort(dim=0, descending=True)\n",
    "        input_seqs = input_seqs[sort_idx]\n",
    "        input_seqs = input_seqs.transpose(0, 1)# [T,B,E]        \n",
    "        embedded = self.embedding(input_seqs)        \n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lens)\n",
    "        outputs, hidden = self.gru(packed,hidden)\n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, :,self.hidden_size:]\n",
    "        _,unsort_idx= sort_idx.sort(dim=0)\n",
    "        outputs = outputs.transpose(0, 1)[unsort_idx].transpose(0, 1).contiguous()\n",
    "        hidden = hidden.transpose(0, 1)[unsort_idx].transpose(0, 1).contiguous()\n",
    "        return outputs, hidden\n",
    "\n",
    "    def init_hidden(self,batch_size):        \n",
    "        result = Variable(torch.zeros(2,batch_size, self.hidden_size))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        stdv = 1. / math.sqrt(self.v.size(0))\n",
    "        self.v.data.normal_(mean=0, std=stdv)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        '''\n",
    "        :param hidden: \n",
    "            previous hidden state of the decoder, in shape (layers*directions,B,H)\n",
    "        :param encoder_outputs:\n",
    "            encoder outputs from Encoder, in shape (T,B,H)\n",
    "        :return\n",
    "            attention energies in shape (B,T)\n",
    "        '''\n",
    "        max_len = encoder_outputs.size(0)\n",
    "        this_batch_size = encoder_outputs.size(1)\n",
    "        H = hidden.repeat(max_len,1,1).transpose(0,1)\n",
    "        encoder_outputs = encoder_outputs.transpose(0,1) # [B*T*H]\n",
    "        attn_energies = self.score(H,encoder_outputs) # compute attention score\n",
    "        return F.softmax(attn_energies).unsqueeze(1) # normalize with softmax\n",
    "\n",
    "    def score(self, hidden, encoder_outputs):\n",
    "        energy = F.tanh(self.attn(torch.cat([hidden, encoder_outputs], 2))) # [B*T*2H]->[B*T*H]\n",
    "        energy = energy.transpose(2,1) # [B*H*T]\n",
    "        v = self.v.repeat(encoder_outputs.data.shape[0],1).unsqueeze(1) #[B*1*H]\n",
    "        energy = torch.bmm(v,energy) # [B*1*T]\n",
    "        return energy.squeeze(1) #[B*T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embed_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super(BahdanauAttnDecoderRNN, self).__init__()\n",
    "        # Define parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size = embed_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.attn = Attn('concat', hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size + embed_size, hidden_size, n_layers, dropout=dropout_p,bidirectional=True)\n",
    "        self.attn_combine = nn.Linear(hidden_size + embed_size, hidden_size)\n",
    "        self.out = nn.Linear(2*hidden_size, output_size)\n",
    "\n",
    "    def forward(self, word_input, last_hidden, encoder_outputs):\n",
    "        '''\n",
    "        :param word_input:\n",
    "            word input for current time step, in shape (B)\n",
    "        :param last_hidden:\n",
    "            last hidden stat of the decoder, in shape (layers*direction*B*H)\n",
    "        :param encoder_outputs:\n",
    "            encoder outputs in shape (T*B*H)\n",
    "        :return\n",
    "            decoder output\n",
    "        Note: we run this one step at a time i.e. you should use a outer loop \n",
    "            to process the whole sequence\n",
    "        Tip(update):\n",
    "        EncoderRNN may be bidirectional or have multiple layers, so the shape of hidden states can be \n",
    "        different from that of DecoderRNN\n",
    "        You may have to manually guarantee that they have the same dimension outside this function,\n",
    "        e.g, select the encoder hidden state of the foward/backward pass.\n",
    "        '''\n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        word_embedded = self.embedding(word_input).view(1, word_input.size(0), -1) # (1,B,V)\n",
    "        word_embedded = self.dropout(word_embedded)\n",
    "        # Calculate attention weights and apply to encoder outputs\n",
    "        attn_weights = self.attn(last_hidden[-1], encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))  # (B,1,V)\n",
    "        context = context.transpose(0, 1)  # (1,B,V)\n",
    "        # Combine embedded input word and attended context, run through RNN\n",
    "        rnn_input = torch.cat((word_embedded, context), 2)\n",
    "#         rnn_input = self.attn_combine(rnn_input) # use it in case your size of rnn_input is different\n",
    "        output, hidden = self.gru(rnn_input, last_hidden)\n",
    "        output = output.squeeze(0)  # (1,B,V)->(B,V)\n",
    "        # context = context.squeeze(0)\n",
    "        # update: \"context\" input before final layer can be problematic.\n",
    "        # output = F.log_softmax(self.out(torch.cat((output, context), 1)))\n",
    "        output = F.log_softmax(self.out(output))\n",
    "        # Return final output, hidden state\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 64\n",
    "hidden_size = 64\n",
    "learning_rate = 0.01\n",
    "encoder = DynamicEncoderRNN(train_dataset.input_voc.n_words,embed_size,hidden_size)\n",
    "decoder = BahdanauAttnDecoderRNN(hidden_size,embed_size,train_dataset.output_voc.n_words)\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "encoder_optimizer_exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(encoder_optimizer, step_size=5, gamma=0.1)\n",
    "decoder_optimizer_exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(decoder_optimizer, step_size=5, gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if torch.cuda.is_available():\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()\n",
    "    criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = torch.load('../models/encoder.pt')\n",
    "decoder = torch.load('../models/decoder.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    total_loss = torch.zeros(1)\n",
    "    total_accuracy = torch.zeros(1)\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    for batch_idx,(input_seqs, input_lens,output_seqs, output_lens) in enumerate(train_dataloader):\n",
    "        input_seqs = input_seqs.long()\n",
    "        output_seqs = output_seqs.long()\n",
    "        encoder_hidden = encoder.init_hidden(len(input_lens))\n",
    "        word_input = torch.Tensor([0]*len(output_lens)).long()\n",
    "        output_seqs = output_seqs.transpose(0,1) # B,T => T,B\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            input_seqs = input_seqs.cuda()\n",
    "            input_lens = input_lens.cuda()\n",
    "            output_seqs = output_seqs.cuda()\n",
    "            output_lens = output_lens.cuda()\n",
    "            encoder_hidden = encoder_hidden.cuda()\n",
    "            word_input = word_input.cuda()\n",
    "            total_loss = total_loss.cuda()\n",
    "            total_accuracy = total_accuracy.cuda()\n",
    "        encoder_outputs,last_hidden = encoder(input_seqs,input_lens,encoder_hidden)\n",
    "        for seq in output_seqs:\n",
    "            word_output,last_hidden = decoder(word_input,last_hidden,encoder_outputs)\n",
    "            word_input = seq\n",
    "            topv, topi = word_output.topk(1)\n",
    "            total_loss += criterion(word_output,word_input).item()\n",
    "            total_accuracy += topi.eq(word_input.view_as(topi)).sum().item()\n",
    "        max_length,batch_size = output_seqs.size()\n",
    "        total_loss /= max_length*batch_size\n",
    "        total_accuracy /= max_length*batch_size\n",
    "#     # print statistics\n",
    "    print(\"-\"*50,\"Test Accuracy:\",round(total_accuracy.item()*100,4),'Test Loss:',round(total_loss.item(),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\chatbot\\lib\\site-packages\\torch\\nn\\functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\chatbot\\lib\\site-packages\\ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\chatbot\\lib\\site-packages\\ipykernel_launcher.py:51: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------- Test Accuracy: 0.0 Test Loss: 0.0254\n"
     ]
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    completed = 0\n",
    "    total_loss = torch.zeros(1)\n",
    "    total_accuracy = torch.zeros(1)\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    encoder_optimizer_exp_lr_scheduler.step()\n",
    "    decoder_optimizer_exp_lr_scheduler.step()\n",
    "    for batch_idx,(input_seqs, input_lens,output_seqs, output_lens) in enumerate(train_dataloader):\n",
    "        \n",
    "        input_seqs = input_seqs.long()\n",
    "        output_seqs = output_seqs.long()\n",
    "        encoder_hidden = encoder.init_hidden(len(input_lens))\n",
    "        word_input = torch.Tensor([0]*len(output_lens)).long()\n",
    "        output_seqs = output_seqs.transpose(0,1) # B,T => T,B\n",
    "        batch_loss = torch.zeros(1)\n",
    "        batch_accuracy = torch.zeros(1)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            input_seqs = input_seqs.cuda()\n",
    "            input_lens = input_lens.cuda()\n",
    "            output_seqs = output_seqs.cuda()\n",
    "            output_lens = output_lens.cuda()\n",
    "            encoder_hidden = encoder_hidden.cuda()\n",
    "            word_input = word_input.cuda()\n",
    "            batch_loss = batch_loss.cuda()\n",
    "            total_loss = total_loss.cuda()\n",
    "            batch_accuracy = batch_accuracy.cuda()\n",
    "            total_accuracy = total_accuracy.cuda()\n",
    "            \n",
    "        encoder_outputs,last_hidden = encoder(input_seqs,input_lens,encoder_hidden)\n",
    "        for seq in output_seqs:\n",
    "            word_output,last_hidden = decoder(word_input,last_hidden,encoder_outputs)\n",
    "            word_input = seq\n",
    "            topv, topi = word_output.topk(1)\n",
    "            batch_loss += criterion(word_output,word_input)\n",
    "            batch_accuracy += topi.eq(word_input.view_as(topi)).sum().item()\n",
    "            \n",
    "        # backward + optimize\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        \n",
    "        max_length,batch_size = output_seqs.size()\n",
    "        batch_loss /= max_length*batch_size\n",
    "        batch_accuracy /= max_length*batch_size\n",
    "        total_loss += batch_loss     \n",
    "        total_accuracy += batch_accuracy\n",
    "        \n",
    "        # print statistics\n",
    "        completed += batch_size\n",
    "        update_progress(completed/len(train_dataset))\n",
    "#         print(\"Batch Accuracy:\",round(batch_accuracy.item()*100,4),'Batch Loss:',round(batch_loss.item(),4))\n",
    "    print(\"-\"*50,\"Train Accuracy:\",round(total_accuracy.item()*100/(batch_idx+1),4),'Train Loss:',round(total_loss.item()/(batch_idx+1),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\chatbot\\lib\\site-packages\\torch\\nn\\functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\chatbot\\lib\\site-packages\\ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\chatbot\\lib\\site-packages\\ipykernel_launcher.py:51: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent: [::::::::::] 100.00% Done...\n",
      "-------------------------------------------------- Train Accuracy: 89.8521 Train Loss: 0.0145\n",
      "Percent: [::::::::::] 100.00% Done...\n",
      "-------------------------------------------------- Train Accuracy: 92.7176 Train Loss: 0.0096\n",
      "Percent: [::::::::::] 100.00% Done...\n",
      "-------------------------------------------------- Train Accuracy: 93.716 Train Loss: 0.0068\n",
      "Percent: [::::::::::] 100.00% Done...\n",
      "-------------------------------------------------- Train Accuracy: 94.6324 Train Loss: 0.0054\n",
      "Percent: [::::::::::] 100.00% Done...\n",
      "-------------------------------------------------- Train Accuracy: 94.8875 Train Loss: 0.0052\n",
      "Percent: [::::::::::] 100.00% Done...\n",
      "-------------------------------------------------- Train Accuracy: 96.8899 Train Loss: 0.003\n",
      "Percent: [::::::::::] 100.00% Done...\n",
      "-------------------------------------------------- Train Accuracy: 97.8106 Train Loss: 0.002\n",
      "Percent: [::::::::::] 100.00% Done...\n",
      "-------------------------------------------------- Train Accuracy: 98.3026 Train Loss: 0.0016\n",
      "Percent: [::::::::::] 100.00% Done...\n",
      "-------------------------------------------------- Train Accuracy: 98.6267 Train Loss: 0.0013\n",
      "Percent: [::::::::::] 100.00% Done...\n",
      "-------------------------------------------------- Train Accuracy: 98.8871 Train Loss: 0.0011\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\chatbot\\lib\\site-packages\\torch\\serialization.py:241: UserWarning: Couldn't retrieve source code for container of type DynamicEncoderRNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\chatbot\\lib\\site-packages\\torch\\serialization.py:241: UserWarning: Couldn't retrieve source code for container of type BahdanauAttnDecoderRNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\chatbot\\lib\\site-packages\\torch\\serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Attn. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(encoder,'../models/encoder.pt')\n",
    "torch.save(decoder,'../models/decoder.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_loader():\n",
    "    test_pred=torch.LongTensor()\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    for batch_idx,(input_seqs, input_lens,output_seqs, output_lens) in enumerate(train_dataloader):\n",
    "        input_seqs = input_seqs.long()\n",
    "        encoder_hidden = encoder.init_hidden(len(input_lens))\n",
    "        word_input = torch.Tensor([0]*len(input_lens)).long()\n",
    "        batch_pred = torch.LongTensor()\n",
    "        if torch.cuda.is_available():\n",
    "            input_seqs = input_seqs.cuda()\n",
    "            input_lens = input_lens.cuda()\n",
    "            encoder_hidden = encoder_hidden.cuda()\n",
    "            word_input = word_input.cuda()\n",
    "            batch_pred = batch_pred.cuda()\n",
    "            test_pred = test_pred.cuda()\n",
    "        encoder_outputs,last_hidden = encoder(input_seqs,input_lens,encoder_hidden)\n",
    "        for _ in range(train_dataset.MAX_LENGTH):\n",
    "            word_output,last_hidden = decoder(word_input,last_hidden,encoder_outputs)\n",
    "            topv, topi = word_output.topk(1)\n",
    "            word_input = topi.squeeze(1)\n",
    "            batch_pred = torch.cat((batch_pred,topi),dim=1)\n",
    "#             if (word_input == 1).sum() == input_seqs.size(0):\n",
    "#                 break\n",
    "        test_pred = torch.cat((test_pred,batch_pred),dim=0)\n",
    "    return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_data(input_seq,input_len):\n",
    "    input_seq,input_len = input_seq.view(1,-1),input_len.view(1)\n",
    "    pred = torch.LongTensor()\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    input_seq = input_seq.long()\n",
    "    encoder_hidden = encoder.init_hidden(len(input_len))\n",
    "    word_input = torch.Tensor([0]*len(input_len)).long()\n",
    "    if torch.cuda.is_available():\n",
    "        input_seq = input_seq.cuda()\n",
    "        input_len = input_len.cuda()\n",
    "        encoder_hidden = encoder_hidden.cuda()\n",
    "        word_input = word_input.cuda()\n",
    "        pred = pred.cuda()\n",
    "    encoder_outputs,last_hidden = encoder(input_seq,input_len,encoder_hidden)\n",
    "    for _ in range(train_dataset.MAX_LENGTH):\n",
    "        word_output,last_hidden = decoder(word_input,last_hidden,encoder_outputs)\n",
    "        topv, topi = word_output.topk(1)\n",
    "        word_input = topi.squeeze(1)\n",
    "        pred = torch.cat((pred,topi),dim=1)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_sentence(line):\n",
    "    line = train_dataset.normalize_string(line)\n",
    "    input_seq = train_dataset.indexes_from_sentence(train_dataset.input_voc,line)+[train_dataset.EOS_token]\n",
    "    input_len = torch.Tensor([len(input_seq)]).int()\n",
    "    input_seq = input_seq + [train_dataset.PAD_token]*(train_dataset.MAX_LENGTH-len(input_seq))\n",
    "    input_seq = torch.Tensor(input_seq).long()\n",
    "    pred = predict_from_data(input_seq,input_len)\n",
    "    input_seq = input_seq.squeeze(0)\n",
    "    pred = pred.squeeze(0)\n",
    "    # Print Input\n",
    "    decoded_words = []\n",
    "    for index in input_seq:\n",
    "        decoded_words.append(train_dataset.input_voc.index2word[index.item()])\n",
    "        if index.item() == train_dataset.EOS_token:\n",
    "            break\n",
    "    print(\"Input:\",\" \".join(decoded_words))\n",
    "    # Print Predict\n",
    "    decoded_words = []\n",
    "    for index in pred:\n",
    "        decoded_words.append(train_dataset.output_voc.index2word[index.item()])\n",
    "        if index.item() == train_dataset.EOS_token:\n",
    "            break\n",
    "    print(\"Predict:\",\" \".join(decoded_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_randomly(index):\n",
    "    input_seq,input_len,output_seq,_ = list(train_dataloader)[index//train_dataloader.batch_size]\n",
    "    idx = index%train_dataloader.batch_size\n",
    "    input_seq,input_len,output_seq = input_seq[idx].view(1,-1),input_len[idx].view(1),output_seq[idx]\n",
    "    predict_from_data(input_seq,input_len)\n",
    "    pred = torch.LongTensor()\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    input_seq = input_seq.long()\n",
    "    encoder_hidden = encoder.init_hidden(len(input_len))\n",
    "    word_input = torch.Tensor([0]*len(input_len)).long()\n",
    "    if torch.cuda.is_available():\n",
    "        input_seq = input_seq.cuda()\n",
    "        input_len = input_len.cuda()\n",
    "        encoder_hidden = encoder_hidden.cuda()\n",
    "        word_input = word_input.cuda()\n",
    "        pred = pred.cuda()\n",
    "    encoder_outputs,last_hidden = encoder(input_seq,input_len,encoder_hidden)\n",
    "    for _ in range(train_dataset.MAX_LENGTH):\n",
    "        word_output,last_hidden = decoder(word_input,last_hidden,encoder_outputs)\n",
    "        topv, topi = word_output.topk(1)\n",
    "        word_input = topi.squeeze(1)\n",
    "        pred = torch.cat((pred,topi),dim=1)\n",
    "        \n",
    "    input_seq = input_seq.squeeze(0)\n",
    "    pred = pred.squeeze(0)\n",
    "    # Print Input\n",
    "    decoded_words = []\n",
    "    for index in input_seq:\n",
    "        decoded_words.append(train_dataset.input_voc.index2word[index.item()])\n",
    "        if index.item() == train_dataset.EOS_token:\n",
    "            break\n",
    "    print(\"Input:\",\" \".join(decoded_words))\n",
    "    # Print Target\n",
    "    decoded_words = []\n",
    "    for index in output_seq:\n",
    "        decoded_words.append(train_dataset.output_voc.index2word[index.item()])\n",
    "        if index.item() == train_dataset.EOS_token:\n",
    "            break\n",
    "    print(\"Target:\",\" \".join(decoded_words))\n",
    "    # Print Predict\n",
    "    decoded_words = []\n",
    "    for index in pred:\n",
    "        decoded_words.append(train_dataset.output_voc.index2word[index.item()])\n",
    "        if index.item() == train_dataset.EOS_token:\n",
    "            break\n",
    "    print(\"Predict:\",\" \".join(decoded_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\chatbot\\lib\\site-packages\\ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\chatbot\\lib\\site-packages\\ipykernel_launcher.py:51: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "prediction = predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\chatbot\\lib\\site-packages\\ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\chatbot\\lib\\site-packages\\ipykernel_launcher.py:51: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: well no , ah i just thought it was one of your other personalities trying to give me a EOS\n",
      "Target: mean guys ? town will just then friends so jesus  wants na sight\" broflovskis draw alain welcome meaning EOS\n",
      "Predict: mean guys ? town will just then friends so jesus jesus  wants na sight\" broflovskis draw alain welcome meaning EOS\n"
     ]
    }
   ],
   "source": [
    "evaluate_randomly(1100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\chatbot\\lib\\site-packages\\ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\chatbot\\lib\\site-packages\\ipykernel_launcher.py:51: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: why EOS\n",
      "Predict: son EOS\n"
     ]
    }
   ],
   "source": [
    "predict_from_sentence(\"why\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus fromatted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\\n'\n",
      "b'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\\n'\n",
      "b'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\\n'\n",
      "b'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?\\n'\n",
      "b\"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\\n\"\n",
      "b'L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow\\n'\n",
      "b\"L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\\n\"\n",
      "b'L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No\\n'\n",
      "b'L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I\\'m kidding.  You know how sometimes you just become this \"persona\"?  And you don\\'t know how to quit?\\n'\n",
      "b'L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?\\n'\n"
     ]
    }
   ],
   "source": [
    "corpus_name = \"cornell movie-dialogs corpus\"\n",
    "corpus = os.path.join(\"../data/interim\", corpus_name)\n",
    "\n",
    "def printLines(file, n=10):\n",
    "    with open(file, 'rb') as datafile:\n",
    "        lines = datafile.readlines()\n",
    "    for line in lines[:n]:\n",
    "        print(line)\n",
    "\n",
    "printLines(os.path.join(corpus, \"movie_lines.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits each line of the file into a dictionary of fields\n",
    "def loadLines(fileName, fields):\n",
    "    lines = {}\n",
    "    with open(fileName, 'r', encoding='iso-8859-1') as f:\n",
    "        for line in f:\n",
    "            values = line.split(\" +++$+++ \")\n",
    "            # Extract fields\n",
    "            lineObj = {}\n",
    "            for i, field in enumerate(fields):\n",
    "                lineObj[field] = values[i]\n",
    "            lines[lineObj['lineID']] = lineObj\n",
    "    return lines\n",
    "\n",
    "\n",
    "# Groups fields of lines from `loadLines` into conversations based on *movie_conversations.txt*\n",
    "def loadConversations(fileName, lines, fields):\n",
    "    conversations = []\n",
    "    with open(fileName, 'r', encoding='iso-8859-1') as f:\n",
    "        for line in f:\n",
    "            values = line.split(\" +++$+++ \")\n",
    "            # Extract fields\n",
    "            convObj = {}\n",
    "            for i, field in enumerate(fields):\n",
    "                convObj[field] = values[i]\n",
    "            # Convert string to list (convObj[\"utteranceIDs\"] == \"['L598485', 'L598486', ...]\")\n",
    "            lineIds = eval(convObj[\"utteranceIDs\"])\n",
    "            # Reassemble lines\n",
    "            convObj[\"lines\"] = []\n",
    "            for lineId in lineIds:\n",
    "                convObj[\"lines\"].append(lines[lineId])\n",
    "            conversations.append(convObj)\n",
    "    return conversations\n",
    "\n",
    "\n",
    "# Extracts pairs of sentences from conversations\n",
    "def extractSentencePairs(conversations):\n",
    "    qa_pairs = []\n",
    "    for conversation in conversations:\n",
    "        # Iterate over all the lines of the conversation\n",
    "        for i in range(len(conversation[\"lines\"]) - 1):  # We ignore the last line (no answer for it)\n",
    "            inputLine = conversation[\"lines\"][i][\"text\"].strip()\n",
    "            targetLine = conversation[\"lines\"][i+1][\"text\"].strip()\n",
    "            # Filter wrong samples (if one of the lists is empty)\n",
    "            if inputLine and targetLine:\n",
    "                qa_pairs.append([inputLine, targetLine])\n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing corpus...\n",
      "\n",
      "Loading conversations...\n",
      "\n",
      "Writing newly formatted file...\n",
      "\n",
      "Sample lines from file:\n",
      "b\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\\r\\r\\n\"\n",
      "b\"Well, I thought we'd start with pronunciation, if that's okay with you.\\tNot the hacking and gagging and spitting part.  Please.\\r\\r\\n\"\n",
      "b\"Not the hacking and gagging and spitting part.  Please.\\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\r\\r\\n\"\n",
      "b\"You're asking me out.  That's so cute. What's your name again?\\tForget it.\\r\\r\\n\"\n",
      "b\"No, no, it's my fault -- we didn't have a proper introduction ---\\tCameron.\\r\\r\\n\"\n",
      "b\"Cameron.\\tThe thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\r\\r\\n\"\n",
      "b\"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\tSeems like she could get a date easy enough...\\r\\r\\n\"\n",
      "b'Why?\\tUnsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\r\\r\\n'\n",
      "b\"Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\tThat's a shame.\\r\\r\\n\"\n",
      "b'Gosh, if only we could find Kat a boyfriend...\\tLet me see what I can do.\\r\\r\\n'\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import csv\n",
    "datafile = os.path.join(corpus, \"formatted_movie_lines.txt\")\n",
    "\n",
    "delimiter = '\\t'\n",
    "# Unescape the delimiter\n",
    "delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n",
    "\n",
    "# Initialize lines dict, conversations list, and field ids\n",
    "lines = {}\n",
    "conversations = []\n",
    "MOVIE_LINES_FIELDS = [\"lineID\", \"characterID\", \"movieID\", \"character\", \"text\"]\n",
    "MOVIE_CONVERSATIONS_FIELDS = [\"character1ID\", \"character2ID\", \"movieID\", \"utteranceIDs\"]\n",
    "\n",
    "# Load lines and process conversations\n",
    "print(\"\\nProcessing corpus...\")\n",
    "lines = loadLines(os.path.join(corpus, \"movie_lines.txt\"), MOVIE_LINES_FIELDS)\n",
    "print(\"\\nLoading conversations...\")\n",
    "conversations = loadConversations(os.path.join(corpus, \"movie_conversations.txt\"),\n",
    "                                  lines, MOVIE_CONVERSATIONS_FIELDS)\n",
    "\n",
    "# Write new csv file\n",
    "print(\"\\nWriting newly formatted file...\")\n",
    "with open(datafile, 'w', encoding='utf-8') as outputfile:\n",
    "    writer = csv.writer(outputfile, delimiter=delimiter)\n",
    "    for pair in extractSentencePairs(conversations):\n",
    "        writer.writerow(pair)\n",
    "\n",
    "# Print a sample of lines\n",
    "print(\"\\nSample lines from file:\")\n",
    "printLines(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default word tokens\n",
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token\n",
    "\n",
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3  # Count SOS, EOS, PAD\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    # Remove words below a certain count threshold\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "\n",
    "        keep_words = []\n",
    "\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print('keep_words {} / {} = {:.4f}'.format(\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "\n",
    "        # Reinitialize dictionaries\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3 # Count default tokens\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preparing training data ...\n",
      "Reading lines...\n",
      "[['can we make this quick ? roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad . again .', 'well i thought we d start with pronunciation if that s okay with you .'], [''], ['well i thought we d start with pronunciation if that s okay with you .', 'not the hacking and gagging and spitting part . please .'], [''], ['not the hacking and gagging and spitting part . please .', 'okay . . . then how bout we try out some french cuisine . saturday ? night ?'], [''], ['you re asking me out . that s so cute . what s your name again ?', 'forget it .'], [''], ['no no it s my fault we didn t have a proper introduction', 'cameron .'], [''], ['cameron .', 'the thing is cameron i m at the mercy of a particularly hideous breed of loser . my sister . i can t date until she does .'], [''], ['the thing is cameron i m at the mercy of a particularly hideous breed of loser . my sister . i can t date until she does .', 'seems like she could get a date easy enough . . .'], [''], ['why ?', 'unsolved mystery . she used to be really popular when she started high school then it was just like she got sick of it or something .'], [''], ['unsolved mystery . she used to be really popular when she started high school then it was just like she got sick of it or something .', 'that s a shame .'], [''], ['gosh if only we could find kat a boyfriend . . .', 'let me see what i can do .'], [''], ['c esc ma tete . this is my head', 'right . see ? you re ready for the quiz .'], [''], ['right . see ? you re ready for the quiz .', 'i don t want to know how to say that though . i want to know useful things . like where the good stores are . how much does champagne cost ? stuff like chat . i have never in my life had to point out my head to someone .'], [''], ['i don t want to know how to say that though . i want to know useful things . like where the good stores are . how much does champagne cost ? stuff like chat . i have never in my life had to point out my head to someone .', 'that s because it s such a nice one .'], [''], ['that s because it s such a nice one .', 'forget french .'], [''], ['how is our little find the wench a date plan progressing ?', 'well there s someone i think might be'], [''], ['there .', 'where ?'], [''], ['you got something on your mind ?', 'i counted on you to help my cause . you and that thug are obviously failing . aren t we ever going on our date ?'], [''], ['you have my word . as a gentleman', 'you re sweet .'], [''], ['how do you get your hair to look like that ?', 'eber s deep conditioner every two days . and i never ever use a blowdryer without the diffuser attachment .'], [''], ['sure have .', 'i really really really wanna go but i can t . not unless my sister goes .'], [''], ['i really really really wanna go but i can t . not unless my sister goes .', 'i m workin on it . but she doesn t seem to be goin for him .'], [''], ['she s not a . . .', 'lesbian ? no . i found a picture of jared leto in one of her drawers so i m pretty sure she s not harboring same sex tendencies .'], [''], ['lesbian ? no . i found a picture of jared leto in one of her drawers so i m pretty sure she s not harboring same sex tendencies .', 'so that s the kind of guy she likes ? pretty ones ?'], [''], ['so that s the kind of guy she likes ? pretty ones ?', 'who knows ? all i ve ever heard her say is that she d dip before dating a guy that smokes .'], [''], ['hi .', 'looks like things worked out tonight huh ?'], ['']]\n",
      "Read 442563 sentence pairs\n",
      "Trimmed to 64271 sentence pairs\n",
      "Counting words...\n",
      "Counted words: 18008\n",
      "\n",
      "pairs:\n",
      "['there .', 'where ?']\n",
      "['you have my word . as a gentleman', 'you re sweet .']\n",
      "['hi .', 'looks like things worked out tonight huh ?']\n",
      "['you know chastity ?', 'i believe we share an art instructor']\n",
      "['have fun tonight ?', 'tons']\n",
      "['well no . . .', 'then that s all you had to say .']\n",
      "['then that s all you had to say .', 'but']\n",
      "['but', 'you always been this selfish ?']\n",
      "['do you listen to this crap ?', 'what crap ?']\n",
      "['what good stuff ?', 'the real you .']\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 10  # Maximum sentence length to consider\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# Read query/response pairs and return a voc object\n",
    "def readVocs(datafile, corpus_name):\n",
    "    print(\"Reading lines...\")\n",
    "    # Read the file and split into lines\n",
    "    lines = open(datafile, encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "    print(pairs[:50])\n",
    "    voc = Voc(corpus_name)\n",
    "    return voc, pairs\n",
    "\n",
    "# Returns True iff both sentences in a pair 'p' are under the MAX_LENGTH threshold\n",
    "def filterPair(p):\n",
    "    # Input sequences need to preserve the last word for EOS token\n",
    "    try:\n",
    "        return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Filter pairs using filterPair condition\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "# Using the functions defined above, return a populated voc object and pairs list\n",
    "def loadPrepareData(corpus, corpus_name, datafile, save_dir):\n",
    "    print(\"Start preparing training data ...\")\n",
    "    voc, pairs = readVocs(datafile, corpus_name)\n",
    "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        voc.addSentence(pair[0])\n",
    "        voc.addSentence(pair[1])\n",
    "    print(\"Counted words:\", voc.num_words)\n",
    "    return voc, pairs\n",
    "\n",
    "\n",
    "# Load/Assemble voc and pairs\n",
    "save_dir = os.path.join(\"data\", \"save\")\n",
    "voc, pairs = loadPrepareData(corpus, corpus_name, datafile, save_dir)\n",
    "# Print some pairs to validate\n",
    "print(\"\\npairs:\")\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
