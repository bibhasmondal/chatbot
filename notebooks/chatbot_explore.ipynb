{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import math\n",
    "import glob\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_config.py\n",
    "MODEL_PREFIX = 'seq2seq_translate'\n",
    "CHECKPOINT_DIR = './checkpoints'\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "teacher_forcing_ratio = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_util.py\n",
    "def load_previous_model(encoder, decoder, checkpoint_dir, model_prefix):\n",
    "    \"\"\"\n",
    "    this can generally used in PyTorch to load previous model,\n",
    "    this function will find max epoch from checkpoints dir, for other models\n",
    "    just change model load format.\n",
    "    :param encoder:\n",
    "    :param decoder:\n",
    "    :param checkpoint_dir:\n",
    "    :param model_prefix:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    f_list = glob.glob(os.path.join(checkpoint_dir, model_prefix) + '-*.pth')\n",
    "    start_epoch = 1\n",
    "    if len(f_list) >= 1:\n",
    "        epoch_list = [int(i.split('-')[-1].split('.')[0]) for i in f_list]\n",
    "        last_checkpoint = f_list[np.argmax(epoch_list)]\n",
    "        if os.path.exists(last_checkpoint):\n",
    "            print('load from {}'.format(last_checkpoint))\n",
    "            model_state_dict = torch.load(last_checkpoint, map_location=lambda storage, loc: storage)\n",
    "            encoder.load_state_dict(model_state_dict['encoder'])\n",
    "            decoder.load_state_dict(model_state_dict['decoder'])\n",
    "            start_epoch = np.max(epoch_list)\n",
    "    return encoder, decoder, start_epoch\n",
    "\n",
    "\n",
    "def save_model(encoder, decoder, checkpoint_dir, model_prefix, epoch, max_keep=5):\n",
    "    \"\"\"\n",
    "    this method can be used in PyTorch to save model,\n",
    "    this will save model with prefix and epochs.\n",
    "    :param encoder:\n",
    "    :param decoder:\n",
    "    :param checkpoint_dir:\n",
    "    :param model_prefix:\n",
    "    :param epoch:\n",
    "    :param max_keep:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    f_list = glob.glob(os.path.join(checkpoint_dir, model_prefix) + '-*.pth')\n",
    "    if len(f_list) >= max_keep + 2:\n",
    "        # this step using for delete the more than 5 and litter one\n",
    "        epoch_list = [int(i.split('-')[-1].split('.')[0]) for i in f_list]\n",
    "        to_delete = [f_list[i] for i in np.argsort(epoch_list)[-max_keep:]]\n",
    "        for f in to_delete:\n",
    "            os.remove(f)\n",
    "    name = model_prefix + '-{}.pth'.format(epoch)\n",
    "    file_path = os.path.join(checkpoint_dir, name)\n",
    "    model_dict = {\n",
    "        'encoder': encoder.state_dict(),\n",
    "        'decoder': decoder.state_dict()\n",
    "    }\n",
    "    torch.save(model_dict, file_path)\n",
    "\n",
    "\n",
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / percent\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loader.py\n",
    "class PairDataLoader(object):\n",
    "    \"\"\"\n",
    "    this class load raw file and generate pair data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.SOS_token = 0\n",
    "        self.EOS_token = 1\n",
    "        self.eng_prefixes = (\n",
    "            \"i am \", \"i m \",\n",
    "            \"he is\", \"he s \",\n",
    "            \"she is\", \"she s\",\n",
    "            \"you are\", \"you re \",\n",
    "            \"we are\", \"we re \",\n",
    "            \"they are\", \"they re \"\n",
    "        )\n",
    "\n",
    "        self._prepare_data('eng', 'fra')\n",
    "\n",
    "    class Lang(object):\n",
    "\n",
    "        def __init__(self, name):\n",
    "            self.name = name\n",
    "            self.word2index = {}\n",
    "            self.word2count = {}\n",
    "            self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "            self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "        def add_sentence(self, sentence):\n",
    "            for word in sentence.split(' '):\n",
    "                self.add_word(word)\n",
    "\n",
    "        def add_word(self, word):\n",
    "            if word not in self.word2index:\n",
    "                self.word2index[word] = self.n_words\n",
    "                self.word2count[word] = 1\n",
    "                self.index2word[self.n_words] = word\n",
    "                self.n_words += 1\n",
    "            else:\n",
    "                self.word2count[word] += 1\n",
    "\n",
    "    def filter_pair(self, p):\n",
    "        return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "               len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "               p[0].startswith(self.eng_prefixes)\n",
    "\n",
    "    def filter_pairs(self, pairs):\n",
    "        return [pair for pair in pairs if self.filter_pair(pair)]\n",
    "\n",
    "    @staticmethod\n",
    "    def unicode_to_ascii(s):\n",
    "        return ''.join(\n",
    "            c for c in unicodedata.normalize('NFD', s)\n",
    "            if unicodedata.category(c) != 'Mn'\n",
    "        )\n",
    "\n",
    "    def normalize_string(self, s):\n",
    "        s = self.unicode_to_ascii(s).lower().strip()\n",
    "        s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "        s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "        return s\n",
    "\n",
    "    def read_lang(self, lang1, lang2, reverse=False):\n",
    "        print(\"Reading lines...\")\n",
    "        lines = open('../data/interim/%s-%s.txt' % (lang1, lang2), encoding='utf-8'). \\\n",
    "            read().strip().split('\\n')\n",
    "        pairs = [[self.normalize_string(s) for s in l.split('\\t')] for l in lines]\n",
    "        if reverse:\n",
    "            pairs = [list(reversed(p)) for p in pairs]\n",
    "            input_lang = self.Lang(lang2)\n",
    "            output_lang = self.Lang(lang1)\n",
    "        else:\n",
    "            input_lang = self.Lang(lang1)\n",
    "            output_lang = self.Lang(lang2)\n",
    "\n",
    "        return input_lang, output_lang, pairs\n",
    "\n",
    "    @staticmethod\n",
    "    def indexes_from_sentence(lang, sentence):\n",
    "        return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "    def variable_from_sentence(self, lang, sentence):\n",
    "        indexes = self.indexes_from_sentence(lang, sentence)\n",
    "        indexes.append(self.EOS_token)\n",
    "        result = Variable(torch.LongTensor(indexes).view(-1, 1))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "    def _prepare_data(self, lang1, lang2, reverse=False):\n",
    "        input_lang, output_lang, pairs = self.read_lang(lang1, lang2, reverse)\n",
    "        print(\"Read %s sentence pairs\" % len(pairs))\n",
    "        self.pairs = self.filter_pairs(pairs)\n",
    "        print(\"Trimmed to %s sentence pairs\" % len(self.pairs))\n",
    "        print(\"Counting words...\")\n",
    "        for pair in self.pairs:\n",
    "            input_lang.add_sentence(pair[0])\n",
    "            output_lang.add_sentence(pair[1])\n",
    "        self.input_lang = input_lang\n",
    "        self.output_lang = output_lang\n",
    "        print(\"Counted words:\")\n",
    "        print(input_lang.name, input_lang.n_words)\n",
    "        print(output_lang.name, output_lang.n_words)\n",
    "\n",
    "    def get_pair_variable(self):\n",
    "        input_variable = self.variable_from_sentence(self.input_lang, random.choice(self.pairs)[0])\n",
    "        target_variable = self.variable_from_sentence(self.output_lang, random.choice(self.pairs)[1])\n",
    "        return input_variable, target_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        embedded = self.embedding(inputs).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        for i in range(self.n_layers):\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        output = self.embedding(inputs).view(1, 1, -1)\n",
    "        for i in range(self.n_layers):\n",
    "            output = F.relu(output)\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, inputs, hidden, encoder_output, encoder_outputs):\n",
    "        embedded = self.embedding(inputs).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)))\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            output = F.relu(output)\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]))\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def init_hidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_loader, input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "                criterion,\n",
    "                max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_variable.size()[0]\n",
    "    print(input_variable.shape)\n",
    "    target_length = target_variable.size()[0]\n",
    "\n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "\n",
    "    loss = 0\n",
    "    try:\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(\n",
    "                input_variable[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] = encoder_output[0][0]\n",
    "    except KeyboardInterrupt:\n",
    "        return\n",
    "\n",
    "    decoder_input = Variable(torch.LongTensor([[data_loader.SOS_token]]))\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        try:\n",
    "            for di in range(target_length):\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_output, encoder_outputs)\n",
    "                loss += criterion(decoder_output, target_variable[di])\n",
    "                decoder_input = target_variable[di]  # Teacher forcing\n",
    "        except KeyboardInterrupt:\n",
    "            return\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        try:\n",
    "            for di in range(target_length):\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_output, encoder_outputs)\n",
    "                topv, topi = decoder_output.data.topk(1)\n",
    "                ni = topi[0][0]\n",
    "\n",
    "                decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "                decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "                loss += criterion(decoder_output, target_variable[di])\n",
    "                if ni == data_loader.EOS_token:\n",
    "                    break\n",
    "        except KeyboardInterrupt:\n",
    "            return\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / target_length\n",
    "\n",
    "\n",
    "def train(data_loader, encoder, decoder, n_epochs, print_every=100, save_every=1000, evaluate_every=100,learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    encoder, decoder, start_epoch = load_previous_model(encoder, decoder, CHECKPOINT_DIR, MODEL_PREFIX)\n",
    "\n",
    "    for epoch in range(start_epoch, n_epochs + 1):\n",
    "\n",
    "        input_variable, target_variable = data_loader.get_pair_variable()\n",
    "\n",
    "        try:\n",
    "            loss = train_model(data_loader, input_variable, target_variable, encoder,\n",
    "                               decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "        print_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (time_since(start, epoch / n_epochs),\n",
    "                                         epoch, epoch / n_epochs * 100, print_loss_avg))\n",
    "\n",
    "        if epoch % save_every == 0:\n",
    "            save_model(encoder, decoder, CHECKPOINT_DIR, MODEL_PREFIX, epoch)\n",
    "\n",
    "        if epoch % evaluate_every == 0:\n",
    "            evaluate_randomly(data_loader, encoder, decoder, n=1)\n",
    "\n",
    "\n",
    "def evaluate(data_loader, encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    input_variable = data_loader.variable_from_sentence(data_loader.input_lang, sentence)\n",
    "    input_length = input_variable.size()[0]\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "\n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_variable[ei],\n",
    "                                                 encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_outputs[ei] + encoder_output[0][0]\n",
    "\n",
    "    decoder_input = Variable(torch.LongTensor([[data_loader.SOS_token]]))  # SOS\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_output, encoder_outputs)\n",
    "        decoder_attentions[di] = decoder_attention.data\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni == data_loader.EOS_token:\n",
    "            decoded_words.append('<EOS>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(data_loader.output_lang.index2word[ni.item()])\n",
    "\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    return decoded_words, decoder_attentions[:di + 1]\n",
    "\n",
    "\n",
    "def evaluate_randomly(data_loader, encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(data_loader.pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(data_loader, encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    pair_data_loader = PairDataLoader()\n",
    "    hidden_size = 256\n",
    "    encoder1 = EncoderRNN(pair_data_loader.input_lang.n_words, hidden_size)\n",
    "    attn_decoder1 = AttnDecoderRNN(hidden_size, pair_data_loader.output_lang.n_words,1, dropout_p=0.1)\n",
    "\n",
    "    if use_cuda:\n",
    "        encoder1 = encoder1.cuda()\n",
    "        attn_decoder1 = attn_decoder1.cuda()\n",
    "    print('start training...')\n",
    "    pair_data_loader.get_pair_variable()\n",
    "    train(pair_data_loader, encoder1, attn_decoder1, 75000)\n",
    "    evaluate_randomly(pair_data_loader, encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 141382 sentence pairs\n",
      "Trimmed to 11132 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 2953\n",
      "fra 4540\n",
      "start training...\n",
      "load from ./checkpoints\\seq2seq_translate-75000.pth\n",
      "torch.Size([6, 1])\n",
      "0m 0s (- 0m 0s) (75000 100%) 0.0332\n",
      "> you re part of the problem .\n",
      "= vous faites partie du probleme .\n",
      "< je suis suis . <EOS>\n",
      "\n",
      "> we re still here .\n",
      "= nous sommes encore ici .\n",
      "<"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\chatbot\\lib\\site-packages\\ipykernel_launcher.py:73: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\chatbot\\lib\\site-packages\\ipykernel_launcher.py:84: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\chatbot\\lib\\site-packages\\ipykernel_launcher.py:65: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " je suis suis . <EOS>\n",
      "\n",
      "> i m back .\n",
      "= je suis revenu .\n",
      "< je suis suis . <EOS>\n",
      "\n",
      "> i am going to write a letter tomorrow .\n",
      "= je vais ecrire une lettre demain .\n",
      "< je suis suis . <EOS>\n",
      "\n",
      "> i m not talking to you tom .\n",
      "= je ne te parle pas tom .\n",
      "< je suis suis . <EOS>\n",
      "\n",
      "> we re relaxed .\n",
      "= nous sommes detendus .\n",
      "< je suis suis . <EOS>\n",
      "\n",
      "> he s my hero .\n",
      "= c est mon heros .\n",
      "< je suis suis . <EOS>\n",
      "\n",
      "> i m not joking .\n",
      "= je ne blague pas .\n",
      "< je suis suis . <EOS>\n",
      "\n",
      "> they re boring .\n",
      "= elles sont ennuyeuses .\n",
      "< je suis suis . <EOS>\n",
      "\n",
      "> i m not wearing socks .\n",
      "= je ne porte pas de chaussettes .\n",
      "< je suis suis . <EOS>\n",
      "\n",
      "> you re very direct .\n",
      "= vous etes tres directe .\n",
      "< je suis suis . <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent: [██████████████████████████████████████████████████] 100% Done...99999%  \n"
     ]
    }
   ],
   "source": [
    "import time, sys\n",
    "\n",
    "# update_progress() : Displays or updates a console progress bar\n",
    "## Accepts a float between 0 and 1. Any int will be converted to a float.\n",
    "## A value under 0 represents a 'halt'.\n",
    "## A value at 1 or bigger represents 100%\n",
    "def update_progress(progress):\n",
    "    barLength = 50 # Modify this to change the length of the progress bar\n",
    "    status = \"\"\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "        status = \"error: progress var must be float\\r\\n\"\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "        status = \"Halt...\\r\\n\"\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "        status = \"Done...\\r\\n\"\n",
    "    block = int(round(barLength*progress))\n",
    "    text = \"\\rPercent: [{0}] {1}% {2}\".format( \"█\"*block + \"_\"*(barLength-block), progress*100, status)\n",
    "    sys.stdout.write(text)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "for i in range(100):\n",
    "    time.sleep(0.1)\n",
    "    update_progress((i+1)/100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-29be49d984ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mChatbotTrainDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mVoc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword2index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "class ChatbotTrainDataset(Dataset):\n",
    "    class Voc(object):\n",
    "        def __init__(self, name):\n",
    "            self.name = name\n",
    "            self.word2index = {}\n",
    "            self.word2count = {}\n",
    "            self.index2word = {0: \"SOS\", 1: \"EOS\", 2:\"PAD\"}\n",
    "            self.n_words = 3  # Count SOS and EOS\n",
    "\n",
    "        def add_sentence(self, sentence):\n",
    "            for word in sentence.split(' '):\n",
    "                self.add_word(word)\n",
    "\n",
    "        def add_word(self, word):\n",
    "            if word not in self.word2index:\n",
    "                self.word2index[word] = self.n_words\n",
    "                self.word2count[word] = 1\n",
    "                self.index2word[self.n_words] = word\n",
    "                self.n_words += 1\n",
    "            else:\n",
    "                self.word2count[word] += 1\n",
    "                \n",
    "    def __init__(self,lang1,lang2,MAX_LENGTH = 10,reverse=False):\n",
    "        self.MAX_LENGTH = MAX_LENGTH\n",
    "        self.SOS_token = 0\n",
    "        self.EOS_token = 1\n",
    "        self.PAD_token = 2\n",
    "        self.input_voc,self.output_voc,pairs = self.prepare_data(lang1,lang2,reverse)\n",
    "        input_data = []\n",
    "        output_data = []\n",
    "        for pair in pairs:\n",
    "            input_data.append(self.indexes_from_sentence(self.input_voc, pair[0])+[self.EOS_token])\n",
    "            output_data.append(self.indexes_from_sentence(self.output_voc, pair[1])+[self.EOS_token])\n",
    "        self.input_data = self.zeroPadding(input_data,self.PAD_token)\n",
    "        self.output_data = self.zeroPadding(output_data,self.PAD_token)       \n",
    "        \n",
    "    def filter_pair(self, p):\n",
    "        return len(p[0].split(' ')) < self.MAX_LENGTH and \\\n",
    "               len(p[1].split(' ')) < self.MAX_LENGTH \n",
    "\n",
    "    def filter_pairs(self, pairs):\n",
    "        return [pair for pair in pairs if self.filter_pair(pair)]\n",
    "\n",
    "    @staticmethod\n",
    "    def unicode_to_ascii(s):\n",
    "        return ''.join(\n",
    "            c for c in unicodedata.normalize('NFD', s)\n",
    "            if unicodedata.category(c) != 'Mn'\n",
    "        )\n",
    "\n",
    "    def normalize_string(self, s):\n",
    "        s = self.unicode_to_ascii(s.strip()).lower().strip()        \n",
    "        s = re.sub(\"([.!?])\", \" \\1\", s)\n",
    "        s = re.sub(\"[^a-zA-Z.!?s]+\", \" \", s)\n",
    "        return s.strip()\n",
    "\n",
    "    def read_lang(self, lang1, lang2, reverse=False):\n",
    "        print(\"Reading lines...\")\n",
    "        # combine every two lines into pairs and normalize\n",
    "        with open('../data/interim/%s-%s.txt' % (lang1, lang2), encoding='utf-8') as f:\n",
    "            content = f.readlines()\n",
    "        lines = [x.strip() for x in content]\n",
    "        it = iter(lines)\n",
    "        pairs = [[self.normalize_string(x), self.normalize_string(next(it))] for x in it]\n",
    "        if reverse:\n",
    "            pairs = [list(reversed(p)) for p in pairs]\n",
    "            input_voc = self.Voc(lang2)\n",
    "            output_voc = self.Voc(lang1)\n",
    "        else:\n",
    "            input_voc = self.Voc(lang1)\n",
    "            output_voc = self.Voc(lang2)\n",
    "        return input_voc, output_voc, pairs\n",
    "\n",
    "    def indexes_from_sentence(self, voc, sentence):\n",
    "        return [voc.word2index[word] for word in sentence.split(' ')]\n",
    "    \n",
    "    # batch_first: true -> false, i.e. shape: seq_len * batch\n",
    "    def zeroPadding(self,data, fillvalue):\n",
    "        pad = len(max(data, key=len))        \n",
    "        return np.array([i + [fillvalue]*(pad-len(i)) for i in data])\n",
    "\n",
    "    def prepare_data(self, lang1, lang2, reverse=False):\n",
    "        input_voc, output_voc, pairs = self.read_lang(lang1, lang2, reverse)\n",
    "        print(\"Read %s sentence pairs\" % len(pairs))\n",
    "        pairs = self.filter_pairs(pairs)\n",
    "        print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "        print(\"Counting words...\")\n",
    "        for pair in pairs:\n",
    "            input_voc.add_sentence(pair[0])\n",
    "            output_voc.add_sentence(pair[1])\n",
    "        print(\"Counted words:\")\n",
    "        print(input_voc.name, input_voc.n_words)\n",
    "        print(output_voc.name, output_voc.n_words)\n",
    "        return input_voc,output_voc,pairs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_data[idx],self.output_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
